# Development of scripts

The scripts work together to collect nodes status from cream and cream-ce. Each script collects node number, node status, 
and load of jobs each node has. After the data has been collected, it is then sent to amazon sqs server using ubuto3 sripts. 
From aws amazon server the data is retrieved by a grid script in a grid server then converts the data to html page that is 
displayed in http://www.grid.core.wits.ac.za/status/index.html. why ? 

# Script functionality

The following documentation applies to both 2ndSend_to_queue.py and Send_table.py scripts.
First we started by importing subprocess library which can help us in getting both pbsnodes and df -h data. Now after getting 
the pbsnodes data we ( who is we)  will store it in a variable named recieve_node_info. Using for and if loops we refine the data obtained 
and select the information we need about the clusters. After retrieving data space, node number, node status, and node jobs we 
store the information to a two dimensional array in a html format then write it in a html file named Output.html and Output2.html 
holding data for both 2ndSend_to_queue.py and Send_table.py respectively. Both  2ndSend_to_queue.py and Send_table.py codes have 
been clearly commented.

Both  2ndSend_to_queue.py and Send_table.py run every 15 minutes  in a screen names every-13-min and every-14-min. The script 
responsible for running  2ndSend_to_queue.py and Send_table.py is every-13-min.sh and every-14-min.sh respectively.
Since  2ndSend_to_queue.py and Send_table.py run every 15 minutes this means Output.html and Output2.html get updated every 15 
minutes. After the 15 minute html files have been updated, submit.py collected the updated information from both output files and 
submit the information to aws Amazon servers using boto3. submit.py runs every 17 minutes, in a screen named submiln and the 
script that is responsible for running it is submit.sh.
After the in formation had arrived in the aws amazon server, it is then retrieved by a scrip named  retrieve_info.py which runs 
every 7-15 minutes as long as there is new data on the amazon server.  retrieve_info.py retrieves the information from aws amazon 
server and writes it in the file named index.html which shows node information in html formal.  retrieve_info.py also runs in a 
screen named every-15-min by a script named every-15-min.sh.
Index.html shows node status and space status in a form of html tables. Index.css is styling for index.html.  
 
